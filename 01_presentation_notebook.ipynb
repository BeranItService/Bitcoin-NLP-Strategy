{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Presentation Notebook CFS Capital\n",
    "Thursday September 20th <br>\n",
    "<br>\n",
    "    Eric Brea Garcia\n",
    "    <br>\n",
    "    Diederik Ketellapper\n",
    "    <br>\n",
    "    Viktor Malesevic\n",
    "    <br>\n",
    "    Daniel Wientjens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the project is to predict whether the bitcoin price is going up or down. For this project we use data from the 1st of May 2017 untill the first of May 2018.\n",
    "\n",
    "We use a variety of sources of information each on a **daily level**:\n",
    "1. All the reddit comments that reference 'bitcoin' for this timeperiod utilizing google bigquery\n",
    "2. Forum Data from bis\n",
    "3. Google Trends data where we retrieve the bitcoin buy/ bitcoin sell +bitcoin buy fraction\n",
    "4. Tweets by the 100 biggest bitcoin influencers of this time period\n",
    "5. Bitcoin actual data from poloniex\n",
    "\n",
    "After this exercise we need to combine the data together to form a master table and then divide this into: Train,Validation and Test.\n",
    "\n",
    "As a penultimate step we use H2O automl to train a variety of models and use an ensemble to train and predict.\n",
    "\n",
    "Then finally we report on our data, what the return of our strategy would be for March and April of 2018.\n",
    "\n",
    "_Please note that not all code of this notebook runs, and it should not be used as such because of the long datadownloads, but it is intended to give a short concise overview of our project and the results._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Reddit Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We downloaded all the reddit comments in the given time period and afterwards performed sentimed analysis and grouped by day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used google cloud for the processing power to download all the reddit comments referencing bitcoin within the time period as below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#standardSQL\n",
    "SELECT created_utc, body\n",
    "\n",
    "FROM(\n",
    "  SELECT\n",
    "    *\n",
    "  FROM\n",
    "    `fh-bigquery.reddit_comments.2017_05` UNION ALL\n",
    "  SELECT\n",
    "    *\n",
    "  FROM\n",
    "    `fh-bigquery.reddit_comments.2017_06` UNION ALL\n",
    "  SELECT\n",
    "    *\n",
    "  FROM\n",
    "    `fh-bigquery.reddit_comments.2017_07` UNION ALL\n",
    "  SELECT\n",
    "    *\n",
    "  FROM\n",
    "    `fh-bigquery.reddit_comments.2017_08` UNION ALL\n",
    "  SELECT\n",
    "    *\n",
    "  FROM\n",
    "    `fh-bigquery.reddit_comments.2017_09` UNION ALL\n",
    "  SELECT\n",
    "    *\n",
    "  FROM\n",
    "    `fh-bigquery.reddit_comments.2017_10` UNION ALL\n",
    "  SELECT\n",
    "    *\n",
    "  FROM\n",
    "    `fh-bigquery.reddit_comments.2017_11` UNION ALL\n",
    "  SELECT\n",
    "    *\n",
    "  FROM\n",
    "    `fh-bigquery.reddit_comments.2017_12` UNION ALL\n",
    "  SELECT\n",
    "    *\n",
    "  FROM\n",
    "    `fh-bigquery.reddit_comments.2018_01` UNION ALL\n",
    "  SELECT\n",
    "    *\n",
    "  FROM\n",
    "    `fh-bigquery.reddit_comments.2018_02` UNION ALL\n",
    "  SELECT\n",
    "    *\n",
    "  FROM\n",
    "    `fh-bigquery.reddit_comments.2018_03` UNION ALL\n",
    "  SELECT\n",
    "    *\n",
    "  FROM\n",
    "    `fh-bigquery.reddit_comments.2018_04`) \n",
    "    \n",
    "WHERE REGEXP_CONTAINS(body, r'bitcoin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the reddit comments\n",
    "reddit = pd.read_csv(\"/Users/Mobile/Downloads/r_comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a cleaning function for the reddit comments\n",
    "def clean(body):\n",
    " return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", body).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the reddit comments and dates\n",
    "reddit['body'] = reddit.body.apply(clean)\n",
    "reddit['date'] = reddit.created_utc.apply(datetime.fromtimestamp)\n",
    "reddit['date'] = reddit.date.dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we perform the sentiment analysis it already computes the magnitude times the sentiment\n",
    "\n",
    "# Import the module\n",
    "from textblob import TextBlob\n",
    "def analize_sentiment(comment):\n",
    "    analysis = TextBlob(comment)\n",
    "    return(analysis.sentiment.polarity)\n",
    "\n",
    "reddit['SA'] = np.array([ analize_sentiment(comment) for comment in reddit['body']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have to group the data by day and we do that by looking at the count of comments and the average sentiment analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by date\n",
    "reddit_groupby= reddit.groupby('date').agg({'body':'count', 'SA':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_groupby = reddit_groupby.rename(columns={'body':'count_comments', 'SA':'mean_sa'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_groupby.to_csv(\"Data/allreddit_nlp.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Forum Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The forum data is a similiar exercise to the reddit comments and is here ommited for brevity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Google Trends Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we download data from Google trends and carry out some of the pre-processing related to it. What we look at is the ratio of \"buy bitcoin\" versus the sum of \"buy bitcoin\" & \"sell bitcoin\" to get a feeling about how the trend is evolving over time. This solves as well one of our problems as google does not output the raw nummer of searches but gives you a number representing the change in your time period. Multiple downloads over multiple timeperiods cannot be compared as it seems that google adds some noise as well. \n",
    "This fraction is a solution but we do not assume it is the most optimal one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "from pytrends.request import TrendReq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the relevant timeframes to search on\n",
    "timeframes2017 = list()\n",
    "timeframesmix = list()\n",
    "timeframes2018 = list()\n",
    "for i in range(5,11):\n",
    "    timeframes2017.append('2017-'+str(i)+'-1 2017-'+str(i+2)+'-1')\n",
    "timeframesmix.append('2017-'+'11'+'-1 2018-'+'1'+'-1')\n",
    "timeframesmix.append('2017-'+'12'+'-1 2018-'+'2'+'-1')\n",
    "for i in range(1,5):\n",
    "    timeframes2018.append('2018-'+str(i)+'-1 2018-'+str(i+2)+'-1')\n",
    "timeframes = timeframes2017+timeframesmix+timeframes2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download using pytrends the referenced timeperiods\n",
    "pytrend = TrendReq(hl='en-US', tz=360)\n",
    "list_google = list()\n",
    "for timeframe in timeframes:\n",
    "    pytrend.build_payload(kw_list=['Buy Bitcoin', 'Sell Bitcoin'], \n",
    "                      timeframe=timeframe)\n",
    "    list_google.append(pytrend.interest_over_time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append all the dataframes to one large one\n",
    "df = pd.DataFrame(list_google[0])\n",
    "\n",
    "for i in range(1,11):\n",
    "    df = df.append(list_google[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the fraction\n",
    "df['googletrends_buy_sell'] = df[\"Buy Bitcoin\"]/(df[\"Sell Bitcoin\"]+df[\"Buy Bitcoin\"])\n",
    "df = df[['googletrends_buy_sell']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the mean for the overlapping dates and output the data\n",
    "df = df.groupby('date').mean().reset_index()\n",
    "df.to_csv(\"Data/google_trends.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Downloading the Twitter Dataset of the 100 biggest influencers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "def getTwitterHandles():\n",
    "    # Fill in with url of page which is to be scraped\n",
    "    url = \"https://cryptoweekly.co/100/\"\n",
    "\n",
    "# Retreives and parses page html\n",
    "    client = urlopen(url)\n",
    "    pageHtml = client.read()\n",
    "    pageSoup = soup(pageHtml, \"html.parser\")\n",
    "\n",
    "# Adds all Twitter handles to twitterHandles list\n",
    "    profiles = pageSoup.findAll(\"div\", {\"class\":\"testimonial-wrapper\"})\n",
    "    twitterHandles = []\n",
    "    for person in profiles:\n",
    "        twitterHandles.append(person.findAll(\"div\",{\"class\":\"author\"}))\n",
    "    for i in range(len(twitterHandles)):\n",
    "        twitterHandles[i]=twitterHandles[i][0].findAll(\"a\")[0].text[1:]\n",
    "\n",
    "    client.close()\n",
    "    return twitterHandles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we download the tweets. Because of the large download please consider carefully to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "# Twitter API credentials (expired, don't even try it)\n",
    "consumer_key = \"\"\n",
    "consumer_secret = \"\"\n",
    "access_key = \"\"\n",
    "access_secret = \"\"\n",
    "\n",
    "\n",
    "def get_all_tweets(screen_name):\n",
    "\tprint(\"Getting tweets from @\" + str(screen_name))\n",
    "\n",
    "\t#Twitter only allows access to a users most recent 3240 tweets with this method\n",
    "\t\n",
    "\t#authorize twitter, initialize tweepy\n",
    "\tauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "\tauth.set_access_token(access_key, access_secret)\n",
    "\tapi = tweepy.API(auth)\n",
    "\t\n",
    "\t#initialize a list to hold all the tweepy Tweets\n",
    "\talltweets = []\t\n",
    "\t\n",
    "\t#make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "\tnew_tweets = api.user_timeline(screen_name = screen_name,count=200)\n",
    "\t\n",
    "\t#save most recent tweets\n",
    "\talltweets.extend(new_tweets)\n",
    "\t\n",
    "\t#save the id of the oldest tweet less one\n",
    "\toldest = alltweets[-1].id - 1\n",
    "\t\n",
    "\t#keep grabbing tweets until there are no tweets left to grab\n",
    "\twhile len(new_tweets) > 0:\n",
    "\t\tprint (\"Getting tweets before %s\" % (oldest))\n",
    "\t\t\n",
    "\t\t#all subsiquent requests use the max_id param to prevent duplicates\n",
    "\t\tnew_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest)\n",
    "\t\t\n",
    "\t\t#save most recent tweets\n",
    "\t\talltweets.extend(new_tweets)\n",
    "\t\t\n",
    "\t\t#update the id of the oldest tweet less one\n",
    "\t\toldest = alltweets[-1].id - 1\n",
    "\t\t\n",
    "\t\tprint (\"...%s tweets downloaded so far\" % (len(alltweets)))\n",
    "\n",
    "\t#transform the tweepy tweets into a 2D array that will populate the csv\t\n",
    "\touttweets = [[tweet.id_str.encode('utf-8'), tweet.created_at.strftime('%m/%d/%Y'), tweet.text.encode('utf-8')] for tweet in alltweets]\n",
    "\t\n",
    "\t#write the csv\t\n",
    "\twith open('Data/Tweets/%s_tweets.csv' % screen_name, 'w') as f:\n",
    "\t\twriter = csv.writer(f)\n",
    "\t\twriter.writerow([\"id\",\"created_at\",\"text\"])\n",
    "\t\twriter.writerows(outtweets)\n",
    "\t\n",
    "\tpass\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\thandles = getTwitterHandles()\n",
    "    # The Peter Todd twitter handle is updated to his new account, as long as the old one was listed in the website:\n",
    "\tfor i in range(len(handles)):\n",
    "\t\tif handles[i] == 'petertoddbtc':\n",
    "\t\t\thandles[i] = 'peterktodd'\n",
    "\tfor handle in handles:\n",
    "\t\tget_all_tweets(str(handle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading the tweets we do a similar exercise as with the reddit comments.\n",
    "We also remove any tweets that are not in the correct timeframe. This is ommited for brevity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Importing the poloniex data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Importing from Poloniex\n",
    "Poloniex is a cryptocurrency exchange found on https://poloniex.com/exchange, to download the data we use a package made by a poloniex community member."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import poloniex\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing from API\n",
    "\n",
    "To import poloniex we need to install the package in the console using:\n",
    "pip install poloniex\n",
    "\n",
    "The help function gives a list of functions included in the package and some descriptions\n",
    "help(poloniex.poloniex)\n",
    "\n",
    "We are using the public data so no keys are needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package has many functionalities but the information we want is public, therefore we do not need access tokens or secret keys. The currency we are looking for is BitCoin relative to the USD, therefore looking at the documentation we know that we need to search using the term 'USDT_BTC', with an additional parameter of seconds between measurements. We chose to take the 24 hours frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "polo = poloniex.Poloniex()\n",
    "btc86400 = pd.DataFrame(polo.returnChartData(\"USDT_BTC\", 86400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Pre-processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the unix timestamp to datetime:\n",
    "btc86400.date = pd.to_datetime(btc86400.date, unit = 's')\n",
    "#Setting the date as index:\n",
    "btc86400.set_index('date', inplace = True)\n",
    "#Taking the period 2017-05-01 to 2018-05-02 (one more day for lagged purposes):\n",
    "btc_data = btc86400[\"2017-04-16\":\"2018-05-02\"]\n",
    "#We decide to take only close and volume since the other columns are extremly correlated to these and may not provide more useful information for our purpose.\n",
    "returns_data = btc_data[[\"close\"]]\n",
    "#Now let's look at the returns (indeed, the close price itself is not particularly interesting):\n",
    "returns_data = returns_data.pct_change()\n",
    "volatility_data = returns_data.rolling(14).std()\n",
    "#We are going to add the volatility of the last 14 days:\n",
    "volatility_data = volatility_data.rename(columns = {'close': 'volatility_14'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final changes for clarity\n",
    "returns_data = returns_data.shift(-1)\n",
    "returns_data = returns_data.dropna()\n",
    "returns_data.columns = [\"return_day+1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we put back together the returns at day+1 with the close and volume information from earlier. We also dropna() for deleting the day 2018-05-02.\n",
    "data = pd.concat((returns_data, btc_data[[\"close\",\"volume\"]]), axis = 1)\n",
    "data = pd.concat((data, volatility_data), axis = 1)\n",
    "data = data.dropna()\n",
    "# filter\n",
    "data = data[\"may-2017\":\"may-2018\"]\n",
    "data.to_csv(\"Data/poloniex_data.csv\")\n",
    "#Creating a csv file for later purposes and checking that reading it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files:\n",
    "poloniex = pd.read_csv(\"Data/poloniex_data.csv\")\n",
    "ggtrends = pd.read_csv(\"Data/google_trends.csv\")\n",
    "twitter = pd.read_csv(\"Data/twitter_agg_ddb.csv\")\n",
    "reddit = pd.read_csv(\"Data/allreddit_nlp.csv\")\n",
    "forum = pd.read_csv(\"Data/merit_compound.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit of preprocessing:\n",
    "# GOOGLE TRENDS:\n",
    "ggtrends = ggtrends.loc[:, ~ggtrends.columns.str.contains('^Unnamed')]\n",
    "# REDDIT:\n",
    "reddit = reddit.loc[:, ~reddit.columns.str.contains('^Unnamed')]\n",
    "reddit = reddit.rename(columns={'date_notime': 'date', 'count_comments': 'Reddit Comments (#)', 'mean_sa': 'Reddit Average SA'})\n",
    "# TWITTER:\n",
    "twitter = twitter.rename(columns={'created_at': 'date','Average SA': 'Twitter Average SA'})\n",
    "twitter['date'] =  pd.to_datetime(twitter['date'], format='%Y%m%d %H:%M:%S')\n",
    "twitter['date'] = twitter['date'].dt.date\n",
    "twitter['date'] = twitter['date'].apply(str)\n",
    "\n",
    "# FORUM:\n",
    "forum = forum.loc[:, ~forum.columns.str.contains('^Unnamed')]\n",
    "forum = forum.rename(columns={'newdate': 'date', 'compound': 'Forum SA Merit', 'merit_compound': 'Forum SA Merit (weighted)'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the datasets into one Master Table:\n",
    "master = pd.merge(poloniex, ggtrends, how = 'inner', on = 'date')\n",
    "master = pd.merge(master, twitter, how = 'outer', on = 'date')\n",
    "master = pd.merge(master, forum, how = 'inner', on = 'date')\n",
    "master = pd.merge(master, reddit, how = 'inner', on = 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the UP/DOWN class:\n",
    "master['invest'] = master['return_day+1']>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
